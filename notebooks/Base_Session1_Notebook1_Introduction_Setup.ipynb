{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaTOQHWoiFra"
   },
   "source": [
    "# Session 1.1: Introduction to LangChain and Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENCn4dTyvYTz"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1BUqB9tjJ7sTE9OKW-he5LwKpkoKBAZgL?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RapYFXmgvapC"
   },
   "source": [
    "\n",
    "## Welcome to the LangChain Certification Course!\n",
    "\n",
    "### What is LangChain?\n",
    "\n",
    "**LangChain** is an open-source framework for building applications powered by Large Language Models (LLMs). It provides:\n",
    "\n",
    "- **Modular components** for working with LLMs\n",
    "- **Pre-built chains** for common use cases\n",
    "- **Integrations** with 100+ LLM providers and data sources\n",
    "- **Tools** for creating agents, RAG systems, and complex workflows\n",
    "\n",
    "### The LangChain Ecosystem\n",
    "\n",
    "1. **LangChain** (Core Library): Building blocks for LLM applications\n",
    "2. **LangSmith**: Observability, debugging, and evaluation platform\n",
    "3. **LangGraph**: Framework for building stateful, multi-actor applications with complex workflows\n",
    "4. **LangServe**: Deploy LangChain runnables and chains as REST APIs\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand what LangChain is and when to use it\n",
    "- Set up your development environment\n",
    "- Make your first LLM call using LangChain\n",
    "- Understand the LangChain ecosystem components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f9A1GamiFre"
   },
   "source": [
    "## 1. Installation\n",
    "\n",
    "Let's install the required packages for this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 24260,
     "status": "ok",
     "timestamp": 1761234514133,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     },
     "user_tz": -240
    },
    "id": "b8XUwTiwiFre"
   },
   "outputs": [],
   "source": [
    "# Install LangChain and related packages\n",
    "!pip install -q langchain==1.0.0 langchain-openai langchain-core langchain-community\n",
    "!pip install -q python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZuFjF82iFrg"
   },
   "source": [
    "## 2. Environment Setup\n",
    "\n",
    "Create a `.env` file in your project directory with your API keys:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "LANGSMITH_API_KEY=your_langsmith_api_key_here\n",
    "LANGSMITH_TRACING=true\n",
    "```\n",
    "\n",
    "**Getting API Keys:**\n",
    "- OpenAI: https://platform.openai.com/api-keys\n",
    "- LangSmith: https://smith.langchain.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 706,
     "status": "ok",
     "timestamp": 1761234514847,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     },
     "user_tz": -240
    },
    "id": "AX5ekk6CiFrh",
    "outputId": "0a36467b-4fdd-4fc2-f259-72cb5bb23642"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI API key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Set OpenAI API key from Google Colab's user environment or default\n",
    "def set_openai_api_key(default_key: str = \"YOUR_API_KEY\") -> None:\n",
    "    \"\"\"Set the OpenAI API key from Google Colab's user environment or use a default value.\"\"\"\n",
    "    #if not (userdata.get(\"OPENAI_API_KEY\") or \"OPENAI_API_KEY\" in os.environ):\n",
    "    try:\n",
    "      os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"MDX_OPENAI_API_KEY\")\n",
    "    except:\n",
    "      os.environ[\"OPENAI_API_KEY\"] = default_key\n",
    "\n",
    "set_openai_api_key()\n",
    "#set_openai_api_key(\"sk-...\")\n",
    "\n",
    "# Verify API key is loaded\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"✅ OpenAI API key loaded successfully!\")\n",
    "else:\n",
    "    print(\"❌ OpenAI API key not found. Please set it in .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVJozWj4iFrh"
   },
   "source": [
    "## 3. Your First LLM Call with LangChain\n",
    "\n",
    "Let's make a simple call to an LLM using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19484,
     "status": "ok",
     "timestamp": 1761234534339,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     },
     "user_tz": -240
    },
    "id": "Dsm4K8DsiFri",
    "outputId": "cd2da909-805a-439a-e997-464798183462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is an open-source framework that helps developers build applications powered by large language models by providing modular components (prompts, chains, agents, memory, and tool integrations) to manage interactions and connect to external tools.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,  # Controls randomness (0-1)\n",
    "    max_tokens=100    # Maximum length of response\n",
    ")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\",\n",
    ")\n",
    "\n",
    "# Make a simple call\n",
    "response = llm.invoke(\"What is LangChain in one sentence?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCqLnE7ZiFri"
   },
   "source": [
    "## 4. Understanding the Response Object\n",
    "\n",
    "LangChain returns structured response objects with metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11645,
     "status": "ok",
     "timestamp": 1761234545974,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     },
     "user_tz": -240
    },
    "id": "p8QsNbOdiFrj",
    "outputId": "c20513bb-17ee-4ddd-8fea-f24d554c6b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "\n",
      "Content: Fun fact: Many modern AI models use something called the \"attention\" mechanism, which is inspired by how humans focus on the most important parts of information. It lets the model weigh different words in a sentence to decide which ones matter most for predicting the next word, enabling it to handle long-range dependencies without reading everything in order.\n",
      "\n",
      "Response Metadata: {'token_usage': {'completion_tokens': 1227, 'prompt_tokens': 13, 'total_tokens': 1240, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1152, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CTrqZHEPSX1Thh7XfR3La1GCmIwQZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}\n",
      "\n",
      "Token Usage: {'completion_tokens': 1227, 'prompt_tokens': 13, 'total_tokens': 1240, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1152, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Examine the response structure\n",
    "response = llm.invoke(\"Tell me a fun fact about AI\")\n",
    "\n",
    "print(\"Response Type:\", type(response))\n",
    "print(\"\\nContent:\", response.content)\n",
    "print(\"\\nResponse Metadata:\", response.response_metadata)\n",
    "print(\"\\nToken Usage:\", response.response_metadata.get('token_usage'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxg4N0sMiFrj"
   },
   "source": [
    "## 5. Chat Models vs LLMs\n",
    "\n",
    "LangChain supports two types of models:\n",
    "\n",
    "- **Chat Models**: Take messages as input, return messages (e.g., GPT-4, Claude)\n",
    "- **LLMs**: Take strings as input, return strings (older generation models)\n",
    "\n",
    "Modern applications typically use **Chat Models**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13547,
     "status": "ok",
     "timestamp": 1761234559530,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     },
     "user_tz": -240
    },
    "id": "l5vL82V5iFrj",
    "outputId": "35ed3018-96e4-44fa-a151-15d093502ae7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An API (Application Programming Interface) is a set of rules that lets one piece of software talk to another.\n",
      "\n",
      "A simple way to think about it:\n",
      "- It’s like a restaurant menu. The menu tells you what you can order and how to ask for it, but you don’t need to know how the food is cooked behind the scenes. The kitchen (the other software) handles that.\n",
      "- The API is the contract that says, “If you ask for X, you’ll get Y.”\n",
      "\n",
      "How it works in practice (high level):\n",
      "1) Your app makes a request to an API (asks for some data or a service).\n",
      "2) The API talks to the service that owns the data or feature.\n",
      "3) The API returns a response with the data or result you asked for.\n",
      "\n",
      "Examples:\n",
      "- A weather app uses a weather service API to get current forecasts.\n",
      "- A maps app uses a map API to show routes and places.\n",
      "\n",
      "Types:\n",
      "- Web APIs: over the internet using HTTP/HTTPS (the common kind you use in apps and websites).\n",
      "- Library or OS APIs: built into a programming language or operating system for developers to use directly in code.\n",
      "\n",
      "Why APIs are useful:\n",
      "- They let different apps and services work together without rebuilding everything from scratch.\n",
      "- They let you reuse existing features and data.\n",
      "- They make it easier to update the backend without breaking apps that rely on it.\n",
      "\n",
      "A couple of quick tips:\n",
      "- Some APIs require keys or tokens to use them and may limit how often you can call them (rate limits).\n",
      "- Always read the documentation to know what requests to send and what responses to expect.\n",
      "\n",
      "In short: an API is a safe, standard way for software to ask another software for data or a service.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Using chat messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful AI assistant specialized in explaining technical concepts.\"),\n",
    "    HumanMessage(content=\"Explain what an API is in simple terms.\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHxP84jaiFrk"
   },
   "source": [
    "## 6. LangSmith Integration (Observability)\n",
    "\n",
    "LangSmith automatically traces your LLM calls when enabled. Visit https://smith.langchain.com to see:\n",
    "\n",
    "- Token usage\n",
    "- Latency metrics\n",
    "- Input/output logs\n",
    "- Cost tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1761234559558,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     },
     "user_tz": -240
    },
    "id": "WLZrz4O2iFrk",
    "outputId": "cdb041ad-2f07-4719-e8f8-357a9da13436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️  LangSmith tracing is disabled\n",
      "Set LANGSMITH_TRACING=true in .env to enable\n"
     ]
    }
   ],
   "source": [
    "# Check if LangSmith tracing is enabled\n",
    "if os.getenv(\"LANGSMITH_TRACING\") == \"true\":\n",
    "    print(\"✅ LangSmith tracing is enabled!\")\n",
    "    print(\"Visit https://smith.langchain.com to view traces\")\n",
    "else:\n",
    "    print(\"ℹ️  LangSmith tracing is disabled\")\n",
    "    print(\"Set LANGSMITH_TRACING=true in .env to enable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJR1pnDUiFrk"
   },
   "source": [
    "## 7. Batch Processing\n",
    "\n",
    "Process multiple inputs efficiently using batch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31282,
     "status": "ok",
     "timestamp": 1761234590835,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     },
     "user_tz": -240
    },
    "id": "MG1ZpOy3iFrk",
    "outputId": "dbfe5b75-cb08-4d44-d21d-238168de3df0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is artificial intelligence?\n",
      "A: Artificial intelligence is the field of computer science focused on making computers perform tasks that normally require human intelligence. These tasks include understanding language, recognizing images and sounds, reasoning, learning from data, planning, and making decisions.\n",
      "\n",
      "Key ideas:\n",
      "- Narrow (weak) AI vs. general (strong) AI: Today’s systems are usually good at specific tasks (e.g., image recognition, language translation). General AI that matches all human cognitive abilities doesn’t yet exist.\n",
      "- How it works: AI systems learn from data using algorithms. They train models on large datasets and then make predictions or decisions on new data.\n",
      "- Common approaches: machine learning (including deep learning), reinforcement learning, sometimes a mix with rule-based (symbolic) methods.\n",
      "- Examples you might know: search and recommendation systems, chatbots, voice assistants, image and speech recognition, autonomous vehicles, fraud detection.\n",
      "- Limitations and caveats: AI systems aren’t truly conscious or human-like; they can be biased or brittle, depend on quality data, and can be hard to interpret.\n",
      "\n",
      "If you want, tell me what context you’re interested in (e.g., everyday apps, how it’s used in business, or how it works under the hood), and I can tailor the explanation.\n",
      "\n",
      "--------------------------------------------------\n",
      "Q: What is LangChain?\n",
      "A: LangChain is an open-source framework for building applications powered by large language models (LLMs). It provides a set of building blocks to structure, orchestrate, and extend LLM-driven workflows, making it easier to turn language models into practical apps.\n",
      "\n",
      "Key ideas and components:\n",
      "- LLM wrappers: easy access to providers like OpenAI, Cohere, HuggingFace, etc.\n",
      "- Prompt templates: reusable prompts with variables and formatting.\n",
      "- Chains: sequences of calls to LLMs and other steps (simple to complex workflows).\n",
      "- Agents and tools: agents can decide what actions to take and call external tools or APIs (via Tools).\n",
      "- Memory: persist and manage context across interactions (e.g., conversations).\n",
      "- Retrieval and vector stores: integrate documents and do retrieval-augmented generation (RAG).\n",
      "- Data ingest and integrations: loaders, databases, and API integrations.\n",
      "- Multi-language support: libraries exist for Python and JavaScript/TypeScript.\n",
      "\n",
      "Typical use cases:\n",
      "- Building chatbots and virtual assistants\n",
      "- Document QA and summarization\n",
      "- Code assistants and copilots\n",
      "- Data-to-text tasks and automated workflows\n",
      "\n",
      "Getting started:\n",
      "- LangChain is not a model by itself; it’s a framework to orchestrate LLMs and data sources.\n",
      "- It’s available for Python and JavaScript/TypeScript (Node.js) and can be installed via pip or npm, with extensive docs and examples available on the official site.\n",
      "\n",
      "--------------------------------------------------\n",
      "Q: Where is Middlesex University Dubai?\n",
      "A: Middlesex University Dubai is in Dubai, United Arab Emirates. The campus is located in Dubai Knowledge Park (Knowledge Village area), near Dubai Media City and Dubai Internet City along Sheikh Zayed Road. \n",
      "\n",
      "If you’d like, I can provide a map link or directions from a specific starting point.\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Batch processing multiple queries\n",
    "questions = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"What is LangChain?\",\n",
    "    \"Where is Middlesex University Dubai?\"\n",
    "]\n",
    "\n",
    "responses = llm.batch(questions)\n",
    "\n",
    "for q, r in zip(questions, responses):\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {r.content}\\n\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wR5NPvkPiFrl"
   },
   "source": [
    "## 8. Streaming Responses\n",
    "\n",
    "Stream responses token-by-token for real-time user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7063,
     "status": "ok",
     "timestamp": 1761234597905,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     },
     "user_tz": -240
    },
    "id": "TpMUrlVziFrl",
    "outputId": "5f712620-715c-461f-a3e5-9ceac1be3238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response...\n",
      "\n",
      "Code streams through the night\n",
      "Loops whisper, bugs fade away\n",
      "Compilers sing soft\n",
      "\n",
      "✅ Streaming complete!\n"
     ]
    }
   ],
   "source": [
    "# Streaming tokens as they arrive\n",
    "print(\"Streaming response...\\n\")\n",
    "\n",
    "for chunk in llm.stream(\"Write a haiku about coding\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n✅ Streaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUdO33_ziFrl"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "✅ What LangChain is and the ecosystem components  \n",
    "✅ How to set up your development environment  \n",
    "✅ Making basic LLM calls with LangChain  \n",
    "✅ Understanding chat messages and response objects  \n",
    "✅ Batch processing and streaming  \n",
    "✅ LangSmith for observability  \n",
    "\n",
    "**Next**: We'll explore different LLM providers and embeddings!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
